SNAKELINES_VERSION = '1.1.5'

import os
import sys
from src.config.dependency_load import load_config, check_dependency
from src.output.copy_files import copy_input_files_with_consisent_names_dict
from src.output.copy_files import copy_config
from src.output.copy_files import store_snakelines_version
from src.output.send_email import send_email
from src.output.report import runtime_report
from src.output.report import multiqc_report
from src.output.report import multiqc_summary_report
from src.output.report import write_output_filenames
from datetime import datetime

from snakemake.io import OutputFiles

# Load configuration file if not already loaded
config = load_config(config)

if 'threads' not in config:
    config['threads'] = 1

# get the snakelines_directory
if 'snakelines_dir' not in config:
    config['snakelines_dir'] = os.path.dirname(os.path.realpath(workflow.snakefile))

if 'sequencing' in config:
    if config['sequencing'] not in ['single_end', 'paired_end']:
        print('ERROR: Invalid sequencing')
        sys.exit(1)
else:
    print('ERROR: Platform not defined')
    sys.exit(1)

if config['sequencing'] == 'single_end':
    if 'platform' not in config:
        print('Single-end mode requires a platform specification (platform: nanopore/illumina)')
        sys.exit(1)

# Run startup scripts that automatically loads imports from src/ and load helper methods
include: config['snakelines_dir'] + '/src/common.snake'

# check dependencies and asses inputs and outputs
outputs, copy_where = check_dependency(config['snakelines_dir'] + '/src/dependency_{}.yaml', config)

# function to evaluate output directories from dependency.yaml
def get_files(source_dict):
    additional_params = {'expand': expand, 'protected': protected, 'temp': temp, 'config': config,
            'pipeline': pipeline, 'directory': directory, 'read_types_with_quality_report': read_types_with_quality_report,
            'multisample_references': multisample_references, 'tax_levels_count_table': tax_levels_count_table,
            'tax_levels_barplot': tax_levels_barplot, 'barplot_formats': barplot_formats, 'pca_formats': pca_formats,
            'tax_levels_alphas': tax_levels_alphas,'map_types_with_quality_report': map_types_with_quality_report,
            'qiime2_alpha_types': qiime2_alpha_types, 'qiime2_beta_types': qiime2_beta_types, 
            'qiime2_beta_significance_columns':qiime2_beta_significance_columns,
            'qiime2_pcoa_custom_columns': qiime2_pcoa_custom_columns,
            'qiime2_heatmap_columns':qiime2_heatmap_columns, 'qiime2_ancom_columns':qiime2_ancom_columns,
            'qiime2_ancom_taxa_lvls':qiime2_ancom_taxa_lvls, 'qiime2_ancom_taxa_columns':qiime2_ancom_taxa_columns}

    files_dict = {}
    for k,v in source_dict.items():
        try:
            files_dict[k] = eval(v, additional_params)
        except (NameError, SyntaxError):
            files_dict[k] = v

    return files_dict

# "pipeline" rule that generates all of the required files



rule pipeline:
    input:
        **get_files(outputs)

onstart:
    pipeline_started = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')
    print('pipeline run started at %s' % pipeline_started, file=sys.stderr)
    print(f'found {len(samples)} samples: {samples}', file=sys.stderr)

onsuccess:
    pipeline_finished = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')
    print('pipeline run finished at %s' % pipeline_finished, file=sys.stderr)

    execution_dir = '%s/_execution/%s' % (config['report_dir'], pipeline_finished)
    summary_dir = '%s/_summary' % config['report_dir']

    if not os.path.exists(execution_dir):
        os.makedirs(execution_dir)

    if not os.path.exists(summary_dir):
        os.makedirs(summary_dir)

    # on success copy the files to report directory and send email(s)
    if 'report_dir' in config:
        print("Analysis is completed - reporting", file=sys.stderr)

        copied = copy_input_files_with_consisent_names_dict(get_files(outputs), get_files(copy_where), pipeline)
        print("Copied {} files to report directory '{}'".format(copied, config['report_dir']), file=sys.stderr)

        copy_config(execution_dir, workflow, pipeline)
        store_snakelines_version(execution_dir, SNAKELINES_VERSION)

        print("Writing output filenames", file=sys.stderr)
        out_list_filename = '%s/rules_output.txt' % execution_dir
        write_output_filenames(out_list_filename, workflow)

        multiqc_config = config.get('report', {}).get('multiqc')
        if multiqc_config:
            print("Creating MultiQC report", file=sys.stderr)
            multiqc_report(
                out_dir=summary_dir,
                out_list_filename=out_list_filename
            )
            if type(multiqc_config) is dict and multiqc_config.get('summary'):
                print("Creating MultiQC summary report", file=sys.stderr)
                lineage_method=config.get('consensus', {}).get('lineage', {}).get('method')
                multiqc_summary_report(
                    multiqc_dir=summary_dir,
                    report_dir=config['report_dir'],
                    sample_references=pipeline.sample_references,
                    metrics=multiqc_config.get('summary'),
                    lineage=lineage_method == 'pangolin'
                )

        print("Generating Runtime Report", file=sys.stderr)
        runtime_report(
            outdir=execution_dir,
            snakefile=workflow.snakefile,
            configfile=workflow.overwrite_configfiles[0]
        )

    send_email(config, get_files(outputs), get_files(copy_where), True)
    print("Finished!", file=sys.stderr)

# on error send email(s)
onerror:
    send_email(config, get_files(outputs), get_files(copy_where), False)
