# TODO: this source should be thoroughly documented
# TODO: method_config should be renamed to method_config

import subprocess
import snakemake
from Bio.Blast import NCBIXML
from Bio import SeqIO
from Bio import SeqUtils
import pysam
import os
import shutil
import numpy as np
import pickle
import pandas as pd
import zlib
pd.set_option('display.max_colwidth', 1000000)
pd.options.display.float_format = '{:.3f}'.format

DEFAULT_MAX_TARGET_SEQS = 10

summary_columns_def = method_config.get('include', [])
if not summary_columns_def:
    summary_columns_def = []

include_coverage  = 'coverage'  in summary_columns_def
include_virfinder = 'virfinder' in summary_columns_def
include_blast     = 'blast'     in summary_columns_def

if include_blast:
    # TODO: change extract_from_config to something like config_for_method('blast')
    blast_references = list(extract_from_config(config, ['classification', 'contig_based', 'reference'], {}).keys())

rule fasta_summary__blast:
    """
    Convert blast results into standardised unified format for fasta summary generation
    :input blast: Blast classification with taxonomy
    :output tsv: Classification in unified format
    """
    input:
        blast = '{indir}/blast/{reference}/{name}.blast.tax.tsv'
    output:
        tsv   = '{indir}/annotation/{name}/attributes/blast/{reference}.blast.tsv'
    params:
        min_query_coverage = method_config.get('min_query_coverage', 0),
        max_target_seqs = method_config.get('max_target_seqs', 999999999)
    run:
        blast = pd.DataFrame.from_csv(input.blast, sep='\t', index_col=None)
        seqids, info_list = [], []
        for seqid, aligns in blast.groupby('qseqid'):
            aligns['qcov'] = ((aligns['qend'] - aligns['qstart']) / aligns['qlen']).abs()
            aligns = aligns[aligns['qcov'] >= params.min_query_coverage]
            aligns = aligns.iloc[:params.max_target_seqs]

            if len(aligns) == 0:
                continue

            def create_link(align):
                return '%3.2f%% - <a href="https://www.ncbi.nlm.nih.gov/nuccore/%s", title="%s">%s</a>' % \
                                  (align['qcov'] * 100, align['sacc'], align['taxonomy'], align['stitle'])

            seqids.append(seqid)
            info_list.append({
                'Homologue title (%s)' % wildcards.reference: ';'.join(aligns['stitle']),
                'Homologue accession (%s)' % wildcards.reference: ';'.join(aligns['stitle']),
                'Homologue link (%s)' % wildcards.reference: '<br />'.join(aligns.apply(create_link, axis=1))
            })

        infos = pd.DataFrame(info_list, index=seqids)
        infos.to_csv(output.tsv, sep='\t')


rule fasta_summary__coverage:
    """
    Extract mapping coverage and store it into standardised unified format for fasta summary generation
    :input qualimap: Quality report statistics generated by Qualimap
    :output tsv: Mapping coverage in standardised unified format for fasta summary generation
    """
    input:
        qualimap = '{indir}/mapping/stats-wgs/{name}/genome_results.txt',
    output:
        tsv      = '{indir}/annotation/{name}/attributes/coverage.tsv'
    run:
        is_seq_coverage_line = False
        seq_coverage_header = '>>>>>>> Coverage per contig'
        seqids, info_list = [], []

        for line in open(input.qualimap):

            line = line.strip()
            if not line:
                continue

            if line.startswith(seq_coverage_header):
                is_seq_coverage_line = True
                continue

            if line.startswith('>>>'):
                is_seq_coverage_line = False
                continue

            if is_seq_coverage_line:
                seqid, seqlen, nbases, coverage, coverage_std = line.split()
                seqids.append(seqid)
                info_list.append({
                    'Mapped bases': nbases,
                    'Coverage': np.round(float(coverage), 2)
                })

        infos = pd.DataFrame(info_list, index=seqids)
        infos.to_csv(output.tsv, sep='\t')

rule fasta_summary__mapped_reads:
    """
    Get number of mapped reads to each reference sequence and convert into standardised unified format for
    fasta summary generation
    :input bam: Mapped reads in bam format
    :input bai: Index for bam file with mapped reads
    :output tsv: Mapped reads per reference sequence in standardised unified format for fasta summary generation
    """
    input:
        bam = '{indir}/mapping/{name}.bam',
        bai = '{indir}/mapping/{name}.bam.bai'
    output:
        tsv = '{indir}/annotation/{name}/attributes/mapped_reads.tsv'
    conda:
        config['snakelines_dir'] + '/enviroments/others_env.yaml'
    shell:
        """
        echo -e '\tMapped reads' > {output.tsv}
        samtools idxstats {input.bam} | cut -f 1,3 | grep -v '*' > {output.tsv}
        """

rule fasta_summary__virfinder:
    input:
        pvalues   = '{indir}/virfinder/{name}.pvalues.tsv'
    output:
        tsv       = '{indir}/annotation/{name}/attributes/virfinder.tsv'
    run:
        pvalues = pd.read_csv(input.pvalues, sep='\t', index_col=0)
        pvalues.index.names = ['']
        pvalues = pvalues[['pvalue']]
        pvalues.columns = ['VirFinder pvalue']
        pvalues.to_csv(output.tsv, sep='\t')


rule fasta_summary__copy_contigs:
    """
    Separate sequences into individual fasta files and calculate basic statistics for each sequence.
    Also store statistics into standardised unified format for fasta summary generation.
    :input fasta: Genomic sequences in fasta format
    :output fasta: Copy of fasta file, stored in the directory accessible from summary HTML report
    :output seqinfo: Statistics of genomic sequences in standardised unified format for fasta summary generation
    """
    input:
        fasta     = '{indir}/{name}.fa'
    output:
        fasta     = '{indir}/annotation/{name}/sequences/all_contigs.fa',
        seqinfo   = '{indir}/annotation/{name}/attributes/seqinfo.fa'
    params:
        indiv_dir = '{indir}/annotation/{name}/sequences'
    run:
        if not os.path.exists(params.indiv_dir):
            os.makedirs(params.indiv_dir)

        shutil.copyfile(input.fasta, output.fasta)

        seqids, info_list = [], []
        for i, seq in enumerate(SeqIO.parse(input.fasta, 'fasta')):

            # Store each sequence in the fasta to the separate file
            with open('%s/%s.fa' % (params.indiv_dir, seq.id), 'w') as out:
                SeqIO.write(seq, out, 'fasta')

            seqids.append(seq.id)
            contig_info = {
                'Length':     len(seq),
                'GC content': str(np.round(SeqUtils.GC(seq.seq), 1)),
                'Compress ratio':  len(zlib.compress(bytes(str(seq.seq), 'utf-8'))) / len(seq.seq)
            }
            info_list.append(contig_info)

        infos = pd.DataFrame(info_list, index=seqids)
        infos.to_csv(output.seqinfo, sep='\t')

def blast_attr_tsvs(wildcards):
    return expand('{indir}/annotation/{name}/attributes/blast/{reference}.blast.tsv',
                   indir=wildcards.indir, name=wildcards.name, reference=blast_references)

rule fasta_summary__summarize_annotations:
    """
    Generate summary HTML report with several aggregated attribute sources from various tools.
    :input template: HTML template with basic report outline
    :input fasta: Analysed sequences in fasta format
    :input attrs_seqinfo: Basic statistics of genomic sequences - length, GC content, complexity...
    :input attrs_coverage: Mapping coverage of sequences
    :input attrs_mapped_reads: Mapped reads per genomic sequence
    :input attrs_virfinder: Probabilities of viral origin for genomic sequences - calculated by the Virfinder tool
    """
    input:
        # Required files to generate blast summary
        template = srcdir('templates/abundances.html'),
        fasta           = '{indir}/annotation/{name}/sequences/all_contigs.fa',
        attrs_seqinfo   = '{indir}/annotation/{name}/attributes/seqinfo.fa',

        # Optional columns of the table
        attrs_blast         =  blast_attr_tsvs                                         if include_blast     else [],
        attrs_coverage      = '{indir}/annotation/{name}/attributes/coverage.tsv'      if include_coverage  else [],
        attrs_mapped_reads  = '{indir}/annotation/{name}/attributes/mapped_reads.tsv'  if include_coverage  else [],
        attrs_virfinder     = '{indir}/annotation/{name}/attributes/virfinder.tsv'     if include_virfinder else [],

    output:
        html = '{indir}/annotation/{name}/summary.html',
        tsv  = '{indir}/annotation/{name}/summary.html.tsv'
    params:
        contig_dir = '{indir}/annotation/sequences',
        max_query_seqs = method_config.get('max_query_seqs', None),
        seqs_per_page = method_config['html'].get('seqs_per_page', 100),
        sort_by = method_config['html'].get('sort_by', ''),
        sort_how = method_config['html'].get('sort_how', 'asc'),
        fasta_dir = 'sequences',
        html_columns = method_config['html'].get('columns', [])
    run:
        def flatten(files):
            flat = []
            for item in files:
                if item:
                    if type(item) == snakemake.io.Namedlist:
                        flat.extend(item)
                    else:
                        flat.append(item)
            return flat

        def load_attrs(attr_file):
            table = pd.DataFrame.from_csv(attr_file, sep='\t', index_col=0)
            return table

        attr_files = flatten([attr_file for name, attr_file in input.__dict__.items() if name.startswith('attrs_')])
        attr_names = [os.path.basename(f)[:-4] for f in attr_files]
        attr_tables = [load_attrs(attr_file) for attr_file in attr_files]

        attrs, last_name = attr_tables[0], attr_names[0]
        for attr_name, attr_table in zip(attr_names[1:], attr_tables[1:]):
            attrs = attrs.merge(attr_table, how='outer',
                                left_index=True, right_index=True)
            last_name = attr_name

        attrs.sort_values(by='Length', ascending=False, inplace=True)
        if params.max_query_seqs:
            attrs = attrs.iloc[:int(params.max_query_seqs)]


        # TSV report
        tsv_columns = [attr for attr in attrs.columns if ' link' not in attr]
        attrs[tsv_columns].to_csv(output.tsv, sep='\t')

        # HTML report
        TEMPLATE = open(input.template).read()
        with open(output.html, 'w') as out:
            attrs['Sequence'] = ['<a href="%s/%s.fa">%s</a>' % (params.fasta_dir, seqid, i+1) \
                                        for i, seqid in enumerate(attrs.index)]
            attrs.index = np.arange(1, len(attrs) + 1)

            html_columns = attrs.columns
            if params.html_columns:
                html_columns = []
                for col_prefix in params.html_columns:
                    html_columns.extend(sorted([col for col in attrs.columns if col.startswith(col_prefix)]))

            html_attrs = attrs[html_columns]
            html_table = html_attrs\
                            .fillna('') \
                            .to_html(escape=False, index=False) \
                            .replace('<table ', '<table id="data" ')

            sort_index = list(html_attrs.columns).index(params.sort_by)
            out.write(TEMPLATE.format(attrs=html_table,
                                      seq_fasta='%s/all_contigs.fa' % params.fasta_dir,
                                      sort_index=sort_index,
                                      sort_how=params.sort_how,
                                      seqs_per_page=params.seqs_per_page))
