from glob import glob
import pandas as pd

def sequencing_library_attribute():
    """
    Infer value for fragment library attribute of Salmon
    https://salmon.readthedocs.io/en/latest/library_type.html
    """
    # User did not provide config, salmon can guess parameters in automatic mode ('A')
    if not 'library' in method_config:
        return 'A'

    # User provided config, infer salmon attribute
    library = method_config['library']
    orientation = {'inward': 'I', 'outward': 'O', 'matching': 'M'}[library['orientation']]
    strandedness = {'stranded': 'S', 'unstranded': 'U'}[library['strandedness']]
    attribute = orientation + strandedness
    if strandedness == 'S':
        direction = {'reverse': 'R', 'forward': 'F'}[library['direction']]
        attribute += direction

    return attribute


# TODO output of classification should be united with the metagenomics read_based approach
rule salmon__classify_reads:
    """
    Find closest homologue sequence for each sequenced fragment
    :input r1: Left side of sequenced fragments in gzipped fastq format
    :input r2: Right side of sequenced fragments in gzipped fastq format
    :input index: One of index files generated by Salmon
    :output counts: TSV file with estimated taxonomic counts for each transcript
    :params library: Fragment library attribute of Salmon
    :params index_dir: Directory for Salmon index files
    :params out_dir: Directory with generated counts
    """
    input:
        r1           = 'reads/%s/{sample}_R1.fastq.gz' % pipeline.preprocessed_read_type,
        r2           = 'reads/%s/{sample}_R2.fastq.gz' % pipeline.preprocessed_read_type,
        index        = 'reference/{reference}/salmon_index/hash.bin'
    output:
        counts       = 'classification/{reference}/samples/{sample}/quant.sf',
    params:
        library      = sequencing_library_attribute(),
        index_dir    = 'reference/{reference}/salmon_index',
        out_dir      = 'classification/{reference}/samples/{sample}'
    log:
        out          = 'classification/{reference}/samples/{sample}/log/salmon.log',
        err          = 'classification/{reference}/samples/{sample}/log/salmon.err'
    threads:
        int(config['threads'])
    conda:
        config['snakelines_dir'] + '/enviroments/others_env.yaml'
    shell:
        '''
        salmon quant \
            --index {params.index_dir} \
            --libType {params.library} \
            --threads {threads} \
            --output {params.out_dir} \
            --mates1 {input.r1} \
            --mates2 {input.r2} \
            --useVBOpt \
            --numBootstraps 30 \
            --seqBias \
            --incompatPrior 0 \
        >  {log.out} \
        2> {log.err}
        '''


rule salmon__create_reference_index:
    """
    Transform transcriptomic sequences into Salmon index for faster classification
    :input fasta: Genomic reference sequences of transcripts
    :output index: One of index files generated by Salmon
    :params index_dir: Directory for generated index files
    """
    input:
        fasta     = 'reference/{reference}/{reference}.transcripts.fa'
    output:
        index     = 'reference/{reference}/salmon_index/hash.bin'
    params:
        index_dir = 'reference/{reference}/salmon_index'
    threads:
        int(config['threads'])
    log:
        out       = 'reference/{reference}/salmon_index/log/db.log',
        err       = 'reference/{reference}/salmon_index/log/db.err'
    conda:
        config['snakelines_dir'] + '/enviroments/others_env.yaml'
    shell:
        '''
        salmon index \
            --transcripts {input.fasta} \
            --index {params.index_dir} \
            --threads {threads} \
         > {log.out} \
        2> {log.err}
        '''

# Conversion from standardised SnakeLines format to Salmon column names
COUNT_TYPES = {
    'tpm': 'TPM',
    'mapped_reads': 'NumReads'
}

rule salmon__prepare_for_krona:
    """
    Convert Salmon classification files into standardised format suitable for generation of Krona reports
    :input counts: Read counts from Salmon
    :output krona: Tabular format suitable for Krona report generation
    :params count_type: Column of counts file that has read counts to use for downstream analysis
    """
    input:
        counts = 'classification/{reference}/samples/{sample}/quant.sf'
    output:
        krona  = 'classification/{reference}/report/krona/individual/{sample}.krn'
    params:
        count_type = COUNT_TYPES[method_config['count_type']]
    run:
        counts = pd.read_csv(input.counts, index_col=None, sep='\t')[[params.count_type, 'Name']]
        counts.to_csv(output.krona, sep='\t', header=None, index=False)


