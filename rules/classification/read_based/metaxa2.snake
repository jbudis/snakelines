include: config['snakelines_dir'] + '/rules/reads/conversion/unzip.snake'

from glob import glob
import pandas as pd

rule metaxa2__classify_reads:
    '''
    Find closest homologue sequence for each sequenced fragment
    :input r1: Left side of sequenced fragments in gzipped fastq format
    :input r2: Right side of sequenced fragments in gzipped fastq format
    :input blast: Blast index of reference sequences (generated by Metaxa2 database builder)
    :input cutoffs: Auxiliary files from reference sequences (generated by Metaxa2 database builder)
    :input hmm: Auxiliary file from reference sequences (generated by Metaxa2 database builder)
    :output taxonomy: Summary taxonomies of classified sequenced fragments
    :params outpref: Prefix of output files
    :params hmm_dir: Directory with HMM models from reference sequences (generated by Metaxa2 database builder)
    :params blast_prefix: Directory with Blast index from reference sequences (generated by Metaxa2 database builder)
    :params confidence: Minimal confidence threshold for taxonomy to be reported
    '''
    input:
        r1           = 'reads/%s/{sample}_R1.fastq.gz' % pipeline.preprocessed_read_type,
        r2           = 'reads/%s/{sample}_R2.fastq.gz' % pipeline.preprocessed_read_type,
        blast        = 'reference/{reference}/metaxa2_index/blast.nhr',
        cutoffs      = 'reference/{reference}/metaxa2_index/blast.cutoffs.txt',
        hmm          = 'reference/{reference}/metaxa2_index/HMMs/E.hmm'
    output:
        taxonomy     = 'classification/{reference}/samples/{sample}/{sample}.taxonomy.txt'
    params:
        outpref      = 'classification/{reference}/samples/{sample}/{sample}',
        hmm_dir      = 'reference/{reference}/metaxa2_index/HMMs',
        blast_prefix = 'reference/{reference}/metaxa2_index/blast',
        confidence = int(100*float(method_config.get('confidence', 0.8)))
    log:
        out          = 'classification/{reference}/samples/{sample}/log/metaxa2.log',
        err          = 'classification/{reference}/samples/{sample}/log/metaxa2.err'
    threads:
        int(config['threads'])
    conda:
        config['snakelines_dir'] + '/enviroments/metaxa_env.yaml'
    shell:
        '''
        metaxa2 \
            -1 {input.r1} \
            -2 {input.r2} \
            --format p \
            -o {params.outpref} \
            -g {wildcards.reference} \
            -R {params.confidence} \
            -p {params.hmm_dir} \
            -d {params.blast_prefix} \
            -T {input.cutoffs} \
            --plus T \
            --cpu {threads} \
        >  {log.out} \
        2> {log.err}
        '''


rule metaxa2__create_reference_index:
    '''
    Transform genomic sequences into Metaxa2 index for faster classification
    :input fasta: Genomic reference sequences in Fasta format
    :input tax: Taxonomies for each reference sequence
    :output blast: Blast index of reference sequences
    :output cutoffs: Auxiliary files from reference sequences
    :output hmm: Auxiliary file from reference sequences
    :params index_dir: Directory for output files
    '''
    input:
        fasta     = 'reference/{reference}/{reference}.fa',
        tax       = 'reference/{reference}/{reference}.tax'
    output:
        blast     = protected('reference/{reference}/metaxa2_index/blast.nhr'),
        cutoffs   = protected('reference/{reference}/metaxa2_index/blast.cutoffs.txt'),
        hmm       = protected('reference/{reference}/metaxa2_index/HMMs/E.hmm'),
    params:
        index_dir = 'reference/{reference}/metaxa2_index'
    threads:
        int(config['threads'])
    log:
        out       = 'reference/{reference}/metaxa2_index/log/db.log',
        err       = 'reference/{reference}/metaxa2_index/log/db.err'
    conda:
        config['snakelines_dir'] + '/enviroments/metaxa_env.yaml'
    shell:
        '''
        metaxa2_dbb \
            -i {input.fasta} \
            -o {params.index_dir} \
            -g {wildcards.reference} \
            -t {input.tax} \
            --auto_rep T \
            --cpu {threads} \
            --plus T \
         > {log.out} \
        2> {log.err}
        '''


rule metaxa2__summarize_classification:
    '''
    Summarize taxonomies per individual taxonomic levels - e.g. for species, order ...
    :input taxonomy: Classified fragments - output of metaxa2 tool
    :input nomatch_template: Auxiliary file for margin case without any classified fragment
    :input nomatch_tax_template: Auxiliary file for margin case without any classified fragment
    :output summary: Summarized taxonomy per species level (others should be generated accordingly)
    :params outpref: Prefix of generated files
    :params nomatch_taxonomy: Auxiliary file for margin case without any classified fragment
    '''
    input:
        taxonomy             = 'classification/{reference}/samples/{sample}/{sample}.taxonomy.txt',
        nomatch_template     = srcdir('metaxa2/nomatch.txt'),
        nomatch_tax_template = srcdir('metaxa2/nomatch.level_7.txt'),
    output:
        summary              = 'classification/{reference}/samples/{sample}/{sample}.level_7.txt',
    params:
        outpref              = 'classification/{reference}/samples/{sample}/{sample}',
        nomatch_taxonomy     = 'classification/{reference}/samples/{sample}/{sample}.taxonomy.nomatch.txt'
    log:
        out                  = 'classification/{reference}/samples/{sample}/log/metaxa2_ttt.log',
        err                  = 'classification/{reference}/samples/{sample}/log/metaxa2_ttt.err'
    conda:
        config['snakelines_dir'] + '/enviroments/metaxa_env.yaml'
    shell:
        '''
        TAXONOMY={input.taxonomy}
        if [ ! -s {input.taxonomy} ];
        then
            cp {input.nomatch_template} {params.nomatch_taxonomy}
            TAXONOMY={params.nomatch_taxonomy}
            echo $TAXONOMY
        fi
        metaxa2_ttt \
                -i $TAXONOMY \
                -o {params.outpref} \
        >  {log.out} \
        2> {log.err}

        if [ ! -f {output.summary} ];
        then
            cp {input.nomatch_tax_template} {output.summary}
        fi
        '''



rule metaxa2__prepare_for_krona:
    '''
    Convert metaxa2 classification files into standardised format suitable for generation of Krona reports
    :input classification: Summarized classification from Metaxa2 classifier
    :output krona: Tabular format suitable for Krona report generation
    '''
    input:
        classification = 'classification/{reference}/samples/{sample}/{sample}.level_7.txt'
    output:
        krona          = 'classification/{reference}/report/krona/individual/{sample}.krn'
    run:

        def most_specialized_classification(wildcards):

            def get_class_level(class_file):
                level = class_file.split('_')[-1].split('.')[0]
                return int(level)

            class_files = glob('classification/{reference}/samples/{sample}/{sample}.level_*.txt'.format(
                                reference=wildcards.reference, sample=wildcards.sample))

            return sorted(class_files, key=get_class_level)[-1]

        invalid_tags = ['unclassified', 'unknown', 'uncultured']

        def is_informative(taxon):
            for tag in invalid_tags:
                if tag in taxon.lower():
                    return False
            return True

        def to_krona_format(tax):
            items = tax.strip(';').split(';')
            for i, item in enumerate(items):
                if not is_informative(item):
                    items = items[:i]
                    break
            return '\t'.join(items)

        counts = pd.read_csv(most_specialized_classification(wildcards), sep='\t', header=None)
        counts.columns = ['tax', 'reads']
        counts['tax_refined'] = counts['tax'].apply(to_krona_format)

        with open(output.krona, 'w') as out:
            for _, row in counts.iterrows():
                out.write('{row.reads}\t{row.tax_refined}\n'.format(row=row))
