import pickle
import pandas as pd
from glob import glob
from Bio import SeqIO

blast_binaries = {
    ('nucleotide', 'nucleotide'): 'blastn',
    ('nucleotide', 'protein'):    'blastx',
    ('protein',    'nucleotide'): 'tblastn',
    ('protein',    'protein'):    'blastp'
}

blast_db_suffices = {
    'nucleotide': 'nsq',
    'protein':    'psq'
}

reference_config = tool_config['reference']

def find_blast_db(wildcards):
    target_type = reference_config[wildcards.reference]['target_type']

    db_suffix = blast_db_suffices[target_type]
    indices = glob('reference/{reference}/blast_index/{reference}.*{db_suffix}' \
                        .format(reference=wildcards.reference, db_suffix=db_suffix))
    if indices:
        return indices

    return 'reference/{reference}/blast_index/{reference}.{db_suffix}' \
                        .format(reference=wildcards.reference, db_suffix=db_suffix)

rule blast__find_homologues:
    '''
    Find homologues for input sequences stored in fasta file in query reference database.
    '''
    input:
        blast_db = find_blast_db,
        fasta   = '{indir}/{name}.fa'
    output:
        tsv     = '{indir}/blast/{reference}/{name}.blast.tsv'
    log:
        out     = '{indir}/blast/{reference}/log/{name}.log',
        err     = '{indir}/blast/{reference}/log/{name}.err'
    params:
        nohead  = '{indir}/blast/{reference}/{name}.blast.tsv.tmp',
        header  = '{indir}/blast/{reference}/{name}.blast.tsv.tmp.header',
        max_target_seqs = lambda wc: reference_config[wc.reference]['max_target_seqs'],
        blast_binary    = lambda wc: blast_binaries[(reference_config[wc.reference]['query_type'],
                                                     reference_config[wc.reference]['target_type'])],
        blast_database  = 'reference/{reference}/blast_index/{reference}'
    threads:
        int(config['threads'])
    shell:
        '''
        set +e pipefail
        HEADER="qseqid \
                sacc staxid stitle \
                pident evalue length mismatch \
                gapopen qstart qend \
                sstart send \
                qlen slen"

        # TODO really ugly way to get header to the blast table, find better way
        HEADER=`echo "$HEADER" | sed -E -e  's/[[:blank:]]+/\t/g'`
        echo $HEADER  > {params.header}
        sed -i 's/\s/\t/g' {params.header}

        {params.blast_binary} \
            -db {params.blast_database} \
            -query {input.fasta} \
            -out {params.nohead} \
            -outfmt "6 `echo $HEADER`" \
            -num_threads {threads} \
            -max_target_seqs {params.max_target_seqs} \
        >  {log.out} \
        2> {log.err}

        cat {params.header} {params.nohead} > {output}
        rm {params.header} {params.nohead}
        '''

# TODO this works for NCBI databases only find a way to load taxonomy directly from blast indices
rule blast__annotate_with_taxonomy:
    input:
        blast = '{indir}/blast/{name}.blast.tsv',
        taxes = '/data/genome/metagenome/blast/nt/17-01-17/taxonomy/code_taxonomy.pickle'
    output:
        blast = '{indir}/blast/{name}.blast.tax.tsv'
    run:
        taxes = pickle.load(open(input.taxes, 'rb'))

        def get_tax(taxid):
            if not taxid or taxid == 'N/A':
                return 'Unknown'
            taxid = int(taxid)
            return ';'.join(taxes.get(taxid, ''))

        with open(input.blast) as in_blast, open(output.blast, 'w') as out_blast:
            for i, line in enumerate(in_blast):
                items = line.strip().split("\t")
                if i == 0:
                    tax_col = items.index('staxid')
                    to_insert = 'taxonomy'
                else:
                    tax = get_tax(items[tax_col])
                    to_insert = tax

                items = items[:tax_col] + [to_insert] + items[tax_col:]
                out_blast.write('\t'.join(items))
                out_blast.write('\n')

# TODO do we need this?
rule blast_filter_by_kingdom:
    input:
        blast = '{indir}/blast/{name}.blast.tax.tsv'
    output:
        blast = '{indir}/blast/{name}.blast.tax.{kingdom, [a-z]+}.tsv'
    run:
        with open(input.blast) as in_blast, open(output.blast, 'w') as out_blast:
            for i, line in enumerate(in_blast):
                items = line.strip().split("\t")
                if i == 0:
                    tax_col = items.index('taxonomy')
                else:
                    tax = items[tax_col].lower()
                    if not tax.startswith(wildcards.kingdom):
                        continue

                out_blast.write(line)

# TODO do we need this?
rule blast_extract_contigs_by_kingdom:
    input:
        blast = '{indir}/blast/{name}.blast.tax.{kingdom}.tsv',
        fasta = '{indir}/{name}.fa'
    output:
        fasta = '{indir}/{name}.blast.tax.{kingdom}.fa'
    run:
        # Store all fasta contigs into memory
        contigs = {contig.id: contig for contig in SeqIO.parse(input.fasta, 'fasta')}

        already_stored = set()
        with open(input.blast) as in_blast, open(output.fasta, 'w') as out_fasta:
            for i, line in enumerate(in_blast):
                items = line.strip().split("\t")
                if i == 0:
                    contig_id_col = items.index('qseqid')
                else:
                    contig_id = items[contig_id_col]
                    if contig_id in already_stored:
                        continue

                    SeqIO.write(contigs[contig_id], out_fasta, 'fasta')
                    already_stored.add(contig_id)


# TODO do we need this?
rule blast_assign_taxonomy_assemblers:
    input:
        fa = '{assembler}/{reference}/{indir}/{sample}.fa',
        blast = '{assembler}/{reference}/{indir}/blast/{reference}/{sample}.blast.tsv',
        tax = 'reference/{reference}/{reference}.tax'
    output:
        tax = '{assembler, matam}/{reference}/{indir, [a-z]+}/{sample}.tax'
    run:
        # Load ids and taxonomies for reference sequences
        taxes = pd.read_csv(input.tax, header=None, index_col=0, sep='\t')[1]

        # Load homologues found by blast
        blast = pd.read_csv(input.blast, index_col=None, sep='\t') \
                  .drop_duplicates(subset=['qseqid'], keep='first') \
                  .set_index('qseqid')

        # Assign taxonomies for best hit homologues to sequences from fasta
        with open(output.tax, 'w') as outtax:
            for query in SeqIO.parse(input.fa, 'fasta'):
                tax = 'Unknown'
                if query.id in blast.index:
                    homologue = blast.loc[query.id]['sacc']
                    tax = taxes[homologue]

                outtax.write('{qid}\t{taxonomy}\n'.format(qid=query.id, taxonomy=tax))

# TODO add reference indices for protein databases as well
rule blast__prepare_reference_index_for_nucleotide:
    '''
    Creates blast indices from provided reference sequences in fasta format.
    '''
    input:
        fa  = 'reference/{reference}/{reference}.fa'
    output:
        nhr = protected('reference/{reference}/blast_index/{reference}.nhr'),
        nin = protected('reference/{reference}/blast_index/{reference}.nin'),
        nsq = protected('reference/{reference}/blast_index/{reference}.nsq')
    log:
        out = 'reference/{reference}/blast_index/log/{reference.log}',
        err = 'reference/{reference}/blast_index/log/{reference.err}'
    params:
        index_prefix = 'reference/{reference}/blast_index/{reference}'
    shell:
        '''
        makeblastdb \
            -in {input.fa} \
            -input_type fasta \
            -dbtype nucl \
            -title {wildcards.reference} \
            -out {params.index_prefix} \
        >  {log.out} \
        2> {log.err}
        '''