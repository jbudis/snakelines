include: config['snake_dir'] + '/classification/classifiers/blast.snake'

import subprocess
from Bio.Blast import NCBIXML
from Bio import SeqIO
import pysam
import os
import numpy as np
import pickle
import pandas as pd
pd.set_option('display.max_colwidth', 1000000)
pd.options.display.float_format = '{:.3f}'.format

DEFAULT_MIN_QUERY_COVERAGE = 0.5
DEFAULT_MAX_TARGET_SEQS = 10

n_contigs = int(config['blast']['report']['max_query_seqs'])
min_similarity   = float(config['blast']['report'].get('min_query_coverage', DEFAULT_MIN_QUERY_COVERAGE))
max_subject_seqs = float(config['blast']['report'].get('max_target_seqs',    DEFAULT_MAX_TARGET_SEQS))

summary_columns_def = config['blast'].get('include', [])
include_coverage  = 'coverage'  in summary_columns_def
include_virfinder = 'virfinder' in summary_columns_def
include_blast     = 'blast'     in summary_columns_def

if include_blast:
    blast_references = list(config['blast']['reference'].keys())

def create_summary(input, output, params):

        # Split contigs.fasta file into separate fasta files, one for each contig - downloadable from HTML page
        if not os.path.exists(params.contig_dir):
            os.mkdir(params.contig_dir)

        contig_lens = {}
        for i, contig in enumerate(SeqIO.parse(input.contigs, 'fasta')):
            with open('%s/%s.fa' % (params.contig_dir, contig.id), 'w') as out:
                SeqIO.write(contig, out, 'fasta')
                contig_lens[contig.id] = len(contig.seq)
            if i >= n_contigs:
                break

        # Collect Blast results
        contig_items = []

        def iter_contigs(blast):
            last_qname, alignments = None, []

            with open(blast) as blast_results:
                for i, line in enumerate(blast_results):
                    items = line.strip().split("\t")
                    if i == 0:
                        tax_col = items.index('taxonomy')
                        gbid_col = items.index('sacc')
                        sname_col = items.index('stitle')
                        qname_col = items.index('qseqid')
                        qstart_col = items.index('qstart')
                        qend_col = items.index('qend')
                        continue

                    qname = items[qname_col]
                    if qname not in contig_lens:
                        continue

                    if last_qname and qname != last_qname:
                        yield (last_qname, contig_lens[last_qname]), alignments
                        last_qname, alignments = qname, []

                    last_qname = qname

                    qcov = (int(items[qend_col]) - int(items[qstart_col])) / contig_lens[qname]
                    if qcov < min_similarity:
                        continue

                    if len(alignments) >= max_subject_seqs:
                        continue

                    alignments.append((items[gbid_col], items[tax_col], items[sname_col], qcov))

            yield (last_qname, contig_lens[last_qname]), alignments

        with open(input.blast) as blast_results:
            for (contig_name, contig_len), alignments in iter_contigs(input.blast):

                links, homologues = [], []
                contig_link = '<a href="individual/%s.fa">%s</a>' % (contig_name, contig_name)
                for gbid, tax, sname, qcov in alignments:
                    links.append('%3.2f%% - <a href="https://www.ncbi.nlm.nih.gov/nuccore/%s", title="%s">%s</a>' % \
                                  (qcov * 100, gbid, tax, sname))
                    homologues.append(sname)

                contig_info = {'Contig (link)': contig_link,
                               'Contig': contig_name,
                               'Homologues (links)': '<br/>'.join(links),
                               'Homologues': ';'.join(homologues),
                               'Length': contig_len,
                               'Contig Name': contig_name}

                contig_items.append(contig_info)

        # Generate report
        contigs = pd.DataFrame(contig_items)
        tsv_columns = ['Contig', 'Length', 'Homologues']

        print(input)
        print(input.remap)
        # Add optional columns
        if input.remap:
            bam = pysam.AlignmentFile(input.remap, 'rb')
            tsv_columns.insert(2, 'Mapped reads')
            contigs['Mapped reads'] = contigs['Contig Name'].apply(lambda contig_name: bam.count(contig=contig_name))
            bam.close()

        if input.pvalues:
            pvalues = pd.read_csv(input.pvalues, sep='\t', index_col=0)
            tsv_columns.insert(2, 'Viral pvalue')
            contigs['Viral pvalue'] = contigs['Contig Name'].apply(lambda cid: pvalues.get_value(index=cid, col='pvalue'))

        # Write TSV report
        contigs[tsv_columns].to_csv(output.table, sep='\t')

        # Write HTML report
        html_columns = ['Homologues (links)' if col == 'Homologues' else col for col in tsv_columns]
        html_columns = ['Contig (link)' if col == 'Contig' else col for col in html_columns]
        contigs = contigs[html_columns]
        contigs.index = np.arange(1, len(contigs) + 1)
        contigs_table = contigs.to_html(escape=False).replace('<table ', '<table id="data" ')

        TEMPLATE = open(input.template).read()
        with open(output.report, 'w') as out:
            out.write(TEMPLATE.format(contigs_table=contigs_table, contigs_fa=params.contig_fa))

        os.system('cp %s %s' % (input.contigs, output.contigs))


rule fasta_summary_blast:
    input:
        blast = '{indir}/blast/{reference}/{name}.blast.tax.tsv'
    output:
        tsv   = '{indir}/fasta_summary/{name}/attributes/blast/{reference}.tsv'
    run:
        pass

rule fasta_summary_coverage:
    input:
        remap     = '{indir}/mapping/{name}.bam',
        remap_idx = '{indir}/mapping/{name}.bam.bai'
    output:
        tsv       = '{indir}/fasta_summary/{name}/attributes/coverage.tsv'
    run:
        pass

rule fasta_summary_virfinder:
    input:
        pvalues   = '{indir}/virfinder/{name}.pvalues.tsv'
    output:
        tsv       = '{indir}/fasta_summary/{name}/attributes/virfinder.tsv'
    run:
        pass


def blast_attr_tsvs(wildcards):
    print(wildcards)
    return expand('{indir}/fasta_summary/{name}/attributes/blast/{reference}.tsv',
                   indir=wildcards.indir, name=wildcards.name, reference=blast_references)

rule fasta_summary:
    input:
        # Required files to generate blast summary
        contigs = '{indir}/{name}.fa',
        template = srcdir('templates/abundances.html'),

        # Optional columns of the table
        attrs_blast     =  blast_attr_tsvs                                        if include_blast     else [],
        attrs_coverage  = '{indir}/fasta_summary/{name}/attributes/coverage.tsv'  if include_coverage  else [],
        attrs_virfinder = '{indir}/fasta_summary/{name}/attributes/virfinder.tsv' if include_virfinder else [],

    output:
        report     = '{indir}/fasta_summary/{name}.html',
        table      = '{indir}/fasta_summary/{name}.html.tsv',
        contigs    = '{indir}/fasta_summary/individual/{name}.fa'
    params:
        contig_dir = '{indir}/fasta_summary/individual',
        contig_fa  = 'individual/{name}.fa'
    run:
        create_summary(input, output, params)



rule blast_assign_taxonomy:
    input:
        fa = '{indir}/{name}.fa',
        blast = '{indir}/blast/{name}.blast',
        tax_db = '/data/genome/metagenome/blast/nt/17-01-17/taxonomy/code_taxonomy.pickle'
    output:
        fa = '{indir}/{name}.tax'
    params:
        blast_db = '/data/genome/metagenome/blast/nt/17-01-17/nt'
    run:
        taxes = pickle.load(open(input.tax_db, 'rb'))
        with open(input.blast) as blast, \
             open(input.fa) as infa, \
             open(output.fa, 'w') as outtax:
            is_first = True
            for i, (contig, sequence) in enumerate(zip(NCBIXML.parse(blast), SeqIO.parse(infa, 'fasta'))):
                assert contig.query.split()[0] == sequence.id, contig.query + ' - ' + sequence.id

                seq_id, taxonomy = sequence.id, ''
                if len(contig.alignments) > 0:
                    for i, alignment in enumerate(contig.alignments):
                        accession = alignment.accession

                        # Retrieve taxid identifier from local Blast database
                        command = 'blastdbcmd -entry {} -db {}  -outfmt "%T"'.format(accession, params.blast_db)
                        output = subprocess.check_output(command.split()).strip().decode('utf-8')
                        taxid = int(output.split('\n')[0].replace('"', ''))
                        if taxid not in taxes:
                            continue
                        align_taxonomy = ';'.join(taxes[taxid])
                        if is_first:
                            taxonomy = align_taxonomy
                            is_first = False
                        is_classified = not ('uncultured' in align_taxonomy.lower() or 'environmental' in align_taxonomy.lower())
                        if is_classified:
                            taxonomy = align_taxonomy
                            break

                outtax.write('{seq_id}\t{taxonomy}\n'.format(seq_id=seq_id, taxonomy=taxonomy))