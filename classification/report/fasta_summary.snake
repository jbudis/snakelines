include: config['snake_dir'] + '/classification/classifiers/blast.snake'

import subprocess
import snakemake
from Bio.Blast import NCBIXML
from Bio import SeqIO
from Bio import SeqUtils
import pysam
import os
import shutil
import numpy as np
import pickle
import pandas as pd
pd.set_option('display.max_colwidth', 1000000)
pd.options.display.float_format = '{:.3f}'.format

DEFAULT_MAX_TARGET_SEQS = 10

summary_columns_def = config['fasta_summary'].get('include', [])
if not summary_columns_def:
    summary_columns_def = []

include_coverage  = 'coverage'  in summary_columns_def
include_virfinder = 'virfinder' in summary_columns_def
include_blast     = 'blast'     in summary_columns_def


if include_blast:
    blast_references = list(config['blast']['reference'].keys())

rule fasta_summary_blast:
    input:
        blast = '{indir}/blast/{reference}/{name}.blast.tax.tsv'
    output:
        tsv   = '{indir}/fasta_summary/{name}/attributes/blast/{reference}.tsv'
    params:
        min_query_coverage = config['fasta_summary'].get('min_query_coverage', 0),
        max_target_seqs = config['fasta_summary'].get('max_target_seqs', 999999999)
    run:
        blast = pd.DataFrame.from_csv(input.blast, sep='\t', index_col=None)
        seqids, info_list = [], []
        for seqid, aligns in blast.groupby('qseqid'):
            aligns['qcov'] = ((aligns['qend'] - aligns['qstart']) / aligns['qlen']).abs()
            aligns = aligns[aligns['qcov'] >= params.min_query_coverage]
            aligns = aligns.iloc[:params.max_target_seqs]

            if len(aligns) == 0:
                continue

            def create_link(align):
                return '%3.2f%% - <a href="https://www.ncbi.nlm.nih.gov/nuccore/%s", title="%s">%s</a>' % \
                                  (align['qcov'] * 100, align['sacc'], align['taxonomy'], align['stitle'])

            seqids.append(seqid)
            info_list.append({
                'Homologue title (%s)' % wildcards.reference: ';'.join(aligns['stitle']),
                'Homologue accession (%s)' % wildcards.reference: ';'.join(aligns['stitle']),
                'Homologue link (%s)' % wildcards.reference: '<br />'.join(aligns.apply(create_link, axis=1))
            })

        infos = pd.DataFrame(info_list, index=seqids)
        infos.to_csv(output.tsv, sep='\t')


rule fasta_summary_coverage:
    input:
        qualimap = '{indir}/mapping/stats-wgs/{name}/genome_results.txt',
    output:
        tsv      = '{indir}/fasta_summary/{name}/attributes/coverage.tsv'
    run:
        is_seq_coverage_line = False
        seq_coverage_header = '>>>>>>> Coverage per contig'
        seqids, info_list = [], []

        for line in open(input.qualimap):

            line = line.strip()
            if not line:
                continue

            if line.startswith(seq_coverage_header):
                is_seq_coverage_line = True
                continue

            if line.startswith('>>>'):
                is_seq_coverage_line = False
                continue

            if is_seq_coverage_line:
                seqid, seqlen, nbases, coverage, coverage_std = line.split()
                seqids.append(seqid)
                info_list.append({
                    'Mapped bases': nbases,
                    'Coverage': np.round(float(coverage), 2)
                })

        infos = pd.DataFrame(info_list, index=seqids)
        infos.to_csv(output.tsv, sep='\t')

rule fasta_summary_virfinder:
    input:
        pvalues   = '{indir}/virfinder/{name}.pvalues.tsv'
    output:
        tsv       = '{indir}/fasta_summary/{name}/attributes/virfinder.tsv'
    run:
        pvalues = pd.read_csv(input.pvalues, sep='\t', index_col=0)
        pvalues.index.names = ['']
        pvalues = pvalues[['pvalue']]
        pvalues.columns = ['VirFinder pvalue']
        pvalues.to_csv(output.tsv, sep='\t')


rule fasta_summary_copy_contigs:
    input:
        fasta     = '{indir}/{name}.fa'
    output:
        fasta     = '{indir}/fasta_summary/{name}/sequences/all_contigs.fa',
        seqinfo   = '{indir}/fasta_summary/{name}/attributes/seqinfo.fa'
    params:
        indiv_dir = '{indir}/fasta_summary/{name}/sequences'
    run:
        if not os.path.exists(params.indiv_dir):
            os.makedirs(params.indiv_dir)

        shutil.copyfile(input.fasta, output.fasta)

        seqids, info_list = [], []
        for i, seq in enumerate(SeqIO.parse(input.fasta, 'fasta')):

            # Store each sequence in the fasta to the separate file
            with open('%s/%s.fa' % (params.indiv_dir, seq.id), 'w') as out:
                SeqIO.write(seq, out, 'fasta')

            seqids.append(seq.id)
            contig_info = {
                'Length':     len(seq),
                'GC content': str(np.round(SeqUtils.GC(seq.seq), 1))
            }
            info_list.append(contig_info)

        infos = pd.DataFrame(info_list, index=seqids)
        infos.to_csv(output.seqinfo, sep='\t')

def blast_attr_tsvs(wildcards):
    return expand('{indir}/fasta_summary/{name}/attributes/blast/{reference}.tsv',
                   indir=wildcards.indir, name=wildcards.name, reference=blast_references)

rule fasta_summary:
    input:
        # Required files to generate blast summary
        template = srcdir('templates/abundances.html'),
        fasta           = '{indir}/fasta_summary/{name}/sequences/all_contigs.fa',
        attrs_seqinfo   = '{indir}/fasta_summary/{name}/attributes/seqinfo.fa',

        # Optional columns of the table
        attrs_blast     =  blast_attr_tsvs                                        if include_blast     else [],
        attrs_coverage  = '{indir}/fasta_summary/{name}/attributes/coverage.tsv'  if include_coverage  else [],
        attrs_virfinder = '{indir}/fasta_summary/{name}/attributes/virfinder.tsv' if include_virfinder else [],

    output:
        html = '{indir}/fasta_summary/{name}/summary.html',
        tsv  = '{indir}/fasta_summary/{name}/summary.html.tsv'
    params:
        contig_dir = '{indir}/fasta_summary/sequences',
        max_query_seqs = config['fasta_summary'].get('max_query_seqs', None),
        sort_by = config['fasta_summary']['html'].get('sort_by', ''),
        sort_how = config['fasta_summary']['html'].get('sort_how', 'asc'),
        fasta_dir = 'sequences',
        html_columns = config['fasta_summary']['html'].get('columns', [])
    run:
        def flatten(files):
            flat = []
            for item in files:
                if item:
                    if type(item) == snakemake.io.Namedlist:
                        flat.extend(item)
                    else:
                        flat.append(item)
            return flat

        def load_attrs(attr_file):
            table = pd.DataFrame.from_csv(attr_file, sep='\t', index_col=0)
            return table

        attr_files = flatten([attr_file for name, attr_file in input.__dict__.items() if name.startswith('attrs_')])
        attr_names = [os.path.basename(f)[:-4] for f in attr_files]
        attr_tables = [load_attrs(attr_file) for attr_file in attr_files]

        attrs, last_name = attr_tables[0], attr_names[0]
        for attr_name, attr_table in zip(attr_names[1:], attr_tables[1:]):
            attrs = attrs.merge(attr_table, how='outer',
                                left_index=True, right_index=True)
            last_name = attr_name

        attrs.sort_values(by='Length', ascending=False, inplace=True)
        if params.max_query_seqs:
            attrs = attrs.iloc[:int(params.max_query_seqs)]


        # TSV report
        tsv_columns = [attr for attr in attrs.columns if ' link' not in attr]
        attrs[tsv_columns].to_csv(output.tsv, sep='\t')

        # HTML report
        TEMPLATE = open(input.template).read()
        with open(output.html, 'w') as out:
            attrs['Sequence'] = ['<a href="%s/%s.fa">%s</a>' % (params.fasta_dir, seqid, seqid) for seqid in attrs.index]
            attrs.index = np.arange(1, len(attrs) + 1)

            html_columns = attrs.columns
            if params.html_columns:
                html_columns = []
                for col_prefix in params.html_columns:
                    html_columns.extend(sorted([col for col in attrs.columns if col.startswith(col_prefix)]))

            html_attrs = attrs[html_columns]
            html_table = html_attrs\
                            .fillna('') \
                            .to_html(escape=False) \
                            .replace('<table ', '<table id="data" ')

            sort_index = list(html_attrs.columns).index(params.sort_by) + 1
            out.write(TEMPLATE.format(attrs=html_table,
                                      seq_fasta='%s/all_contigs.fa' % params.fasta_dir,
                                      sort_index=sort_index, sort_how=params.sort_how))

rule blast_assign_taxonomy:
    input:
        fa = '{indir}/{name}.fa',
        blast = '{indir}/blast/{name}.blast',
        tax_db = '/data/genome/metagenome/blast/nt/17-01-17/taxonomy/code_taxonomy.pickle'
    output:
        fa = '{indir}/{name}.tax'
    params:
        blast_db = '/data/genome/metagenome/blast/nt/17-01-17/nt'
    run:
        taxes = pickle.load(open(input.tax_db, 'rb'))
        with open(input.blast) as blast, \
             open(input.fa) as infa, \
             open(output.fa, 'w') as outtax:
            is_first = True
            for i, (contig, sequence) in enumerate(zip(NCBIXML.parse(blast), SeqIO.parse(infa, 'fasta'))):
                assert contig.query.split()[0] == sequence.id, contig.query + ' - ' + sequence.id

                seq_id, taxonomy = sequence.id, ''
                if len(contig.alignments) > 0:
                    for i, alignment in enumerate(contig.alignments):
                        accession = alignment.accession

                        # Retrieve taxid identifier from local Blast database
                        command = 'blastdbcmd -entry {} -db {}  -outfmt "%T"'.format(accession, params.blast_db)
                        output = subprocess.check_output(command.split()).strip().decode('utf-8')
                        taxid = int(output.split('\n')[0].replace('"', ''))
                        if taxid not in taxes:
                            continue
                        align_taxonomy = ';'.join(taxes[taxid])
                        if is_first:
                            taxonomy = align_taxonomy
                            is_first = False
                        is_classified = not ('uncultured' in align_taxonomy.lower() or 'environmental' in align_taxonomy.lower())
                        if is_classified:
                            taxonomy = align_taxonomy
                            break

                outtax.write('{seq_id}\t{taxonomy}\n'.format(seq_id=seq_id, taxonomy=taxonomy))