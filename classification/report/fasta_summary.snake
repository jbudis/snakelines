include: config['snake_dir'] + '/classification/classifiers/blast.snake'

import subprocess
from Bio.Blast import NCBIXML
from Bio import SeqIO
from Bio import SeqUtils
import pysam
import os
import shutil
import numpy as np
import pickle
import pandas as pd
pd.set_option('display.max_colwidth', 1000000)
pd.options.display.float_format = '{:.3f}'.format

DEFAULT_MAX_TARGET_SEQS = 10

summary_columns_def = config['fasta_summary'].get('include', [])
if not summary_columns_def:
    summary_columns_def = []

include_coverage  = 'coverage'  in summary_columns_def
include_virfinder = 'virfinder' in summary_columns_def
include_blast     = 'blast'     in summary_columns_def


if include_blast:
    blast_references = list(config['blast']['reference'].keys())

def create_summary(input, output, params):

        # Split contigs.fasta file into separate fasta files, one for each contig - downloadable from HTML page
        if not os.path.exists(params.contig_dir):
            os.mkdir(params.contig_dir)



        # Collect Blast results
        contig_items = []

        def iter_contigs(blast):
            last_qname, alignments = None, []

            with open(blast) as blast_results:
                for i, line in enumerate(blast_results):
                    items = line.strip().split("\t")
                    if i == 0:
                        tax_col = items.index('taxonomy')
                        gbid_col = items.index('sacc')
                        sname_col = items.index('stitle')
                        qname_col = items.index('qseqid')
                        qstart_col = items.index('qstart')
                        qend_col = items.index('qend')
                        continue

                    qname = items[qname_col]
                    if qname not in contig_lens:
                        continue

                    if last_qname and qname != last_qname:
                        yield (last_qname, contig_lens[last_qname]), alignments
                        last_qname, alignments = qname, []

                    last_qname = qname

                    qcov = (int(items[qend_col]) - int(items[qstart_col])) / contig_lens[qname]
                    if qcov < min_similarity:
                        continue

                    if len(alignments) >= max_subject_seqs:
                        continue

                    alignments.append((items[gbid_col], items[tax_col], items[sname_col], qcov))

            yield (last_qname, contig_lens[last_qname]), alignments

        with open(input.blast) as blast_results:
            for (contig_name, contig_len), alignments in iter_contigs(input.blast):

                links, homologues = [], []
                contig_link = '<a href="individual/%s.fa">%s</a>' % (contig_name, contig_name)
                for gbid, tax, sname, qcov in alignments:
                    links.append('%3.2f%% - <a href="https://www.ncbi.nlm.nih.gov/nuccore/%s", title="%s">%s</a>' % \
                                  (qcov * 100, gbid, tax, sname))
                    homologues.append(sname)

                contig_info = {'Contig (link)': contig_link,
                               'Contig': contig_name,
                               'Homologues (links)': '<br/>'.join(links),
                               'Homologues': ';'.join(homologues),
                               'Length': contig_len,
                               'Contig Name': contig_name}

                contig_items.append(contig_info)

        # Generate report
        contigs = pd.DataFrame(contig_items)
        tsv_columns = ['Contig', 'Length', 'Homologues']

        print(input)
        print(input.remap)


        # Write TSV report
        contigs[tsv_columns].to_csv(output.table, sep='\t')

        # Write HTML report
        html_columns = ['Homologues (links)' if col == 'Homologues' else col for col in tsv_columns]
        html_columns = ['Contig (link)' if col == 'Contig' else col for col in html_columns]
        contigs = contigs[html_columns]
        contigs.index = np.arange(1, len(contigs) + 1)
        contigs_table = contigs.to_html(escape=False).replace('<table ', '<table id="data" ')

        TEMPLATE = open(input.template).read()
        with open(output.report, 'w') as out:
            out.write(TEMPLATE.format(contigs_table=contigs_table, contigs_fa=params.contig_fa))

        os.system('cp %s %s' % (input.contigs, output.contigs))


rule fasta_summary_blast:
    input:
        blast = '{indir}/blast/{reference}/{name}.blast.tax.tsv'
    output:
        tsv   = '{indir}/fasta_summary/{name}/attributes/blast/{reference}.tsv'
    run:
        pass

rule fasta_summary_coverage:
    input:
        qualimap = '{indir}/mapping/stats-wgs/{name}/genome_results.txt',
    output:
        tsv      = '{indir}/fasta_summary/{name}/attributes/coverage.tsv'
    run:
        is_seq_coverage_line = False
        seq_coverage_header = '>>>>>>> Coverage per contig'
        seqids, info_list = [], []

        for line in open(input.qualimap):

            line = line.strip()
            if not line:
                continue

            if line.startswith(seq_coverage_header):
                is_seq_coverage_line = True
                continue

            if line.startswith('>>>'):
                is_seq_coverage_line = False
                continue

            if is_seq_coverage_line:
                seqid, seqlen, nbases, coverage, coverage_std = line.split()
                seqids.append(seqid)
                info_list.append({
                    'Mapped bases': nbases,
                    'Coverage': np.round(float(coverage), 2)
                })

        infos = pd.DataFrame(info_list, index=seqids)
        infos.to_csv(output.tsv, sep='\t')

rule fasta_summary_virfinder:
    input:
        pvalues   = '{indir}/virfinder/{name}.pvalues.tsv'
    output:
        tsv       = '{indir}/fasta_summary/{name}/attributes/virfinder.tsv'
    run:
        pvalues = pd.read_csv(input.pvalues, sep='\t', index_col=0)
        pvalues.index.names = ['']
        pvalues = pvalues[['pvalue']]
        pvalues.columns = ['VirFinder pvalue']
        pvalues.to_csv(output.tsv, sep='\t')


rule fasta_summary_copy_contigs:
    input:
        fasta     = '{indir}/{name}.fa'
    output:
        fasta     = '{indir}/fasta_summary/{name}.fa',
        seqinfo   = '{indir}/fasta_summary/{name}/attributes/seqinfo.fa'
    params:
        indiv_dir = '{indir}/fasta_summary/individual'
    run:
        if not os.path.exists(params.indiv_dir):
            os.makedirs(params.indiv_dir)

        shutil.copyfile(input.fasta, output.fasta)

        seqids, info_list = [], []
        for i, seq in enumerate(SeqIO.parse(input.fasta, 'fasta')):

            # Store each sequence in the fasta to the separate file
            with open('%s/%s.fa' % (params.indiv_dir, seq.id), 'w') as out:
                SeqIO.write(seq, out, 'fasta')

            seqids.append(seq.id)
            contig_info = {
                'Length':     len(seq),
                'GC content': str(np.round(SeqUtils.GC(seq.seq), 1))
            }
            info_list.append(contig_info)

        infos = pd.DataFrame(info_list, index=seqids)
        infos.to_csv(output.seqinfo, sep='\t')

def blast_attr_tsvs(wildcards):
    return expand('{indir}/fasta_summary/{name}/attributes/blast/{reference}.tsv',
                   indir=wildcards.indir, name=wildcards.name, reference=blast_references)

rule fasta_summary:
    input:
        # Required files to generate blast summary
        template = srcdir('templates/abundances.html'),
        fasta           = '{indir}/fasta_summary/{name}.fa',
        attrs_seqinfo   = '{indir}/fasta_summary/{name}/attributes/seqinfo.fa',

        # Optional columns of the table
        attrs_blast     =  blast_attr_tsvs                                        if include_blast     else [],
        attrs_coverage  = '{indir}/fasta_summary/{name}/attributes/coverage.tsv'  if include_coverage  else [],
        attrs_virfinder = '{indir}/fasta_summary/{name}/attributes/virfinder.tsv' if include_virfinder else [],

    output:
        html = '{indir}/fasta_summary/{name}.html',
        tsv  = '{indir}/fasta_summary/{name}.html.tsv'
    params:
        contig_dir = '{indir}/fasta_summary/individual',
        contig_fa  = 'individual/{name}.fa',
        max_query_seqs = config['fasta_summary'].get('max_query_seqs', None)
    run:
        def flatten(files):
            flat = []
            for item in files:
                if item:
                    if type(item) == list:
                        flat.expand(item)
                    else:
                        flat.append(item)
            return flat

        def load_attrs(attr_file):
            table = pd.DataFrame.from_csv(attr_file, sep='\t', index_col=0)
            return table

        attr_files = flatten([attr_file for name, attr_file in input.__dict__.items() if name.startswith('attrs_')])
        attr_tables = [load_attrs(attr_file) for attr_file in attr_files]

        attrs = attr_tables[0]
        for attr_table in attr_tables[1:]:
            attrs = attrs.merge(attr_table, how='outer', left_index=True, right_index=True)

        attrs.sort_values(by='Length', ascending=False, inplace=True)
        if params.max_query_seqs:
            attrs = attrs.iloc[:int(params.max_query_seqs)]

        print(attrs)

        attrs.to_csv(output.html, sep='\t')
        attrs.to_csv(output.tsv, sep='\t')

ruleorder: fasta_summary_copy_contigs > filter_contigs

rule blast_assign_taxonomy:
    input:
        fa = '{indir}/{name}.fa',
        blast = '{indir}/blast/{name}.blast',
        tax_db = '/data/genome/metagenome/blast/nt/17-01-17/taxonomy/code_taxonomy.pickle'
    output:
        fa = '{indir}/{name}.tax'
    params:
        blast_db = '/data/genome/metagenome/blast/nt/17-01-17/nt'
    run:
        taxes = pickle.load(open(input.tax_db, 'rb'))
        with open(input.blast) as blast, \
             open(input.fa) as infa, \
             open(output.fa, 'w') as outtax:
            is_first = True
            for i, (contig, sequence) in enumerate(zip(NCBIXML.parse(blast), SeqIO.parse(infa, 'fasta'))):
                assert contig.query.split()[0] == sequence.id, contig.query + ' - ' + sequence.id

                seq_id, taxonomy = sequence.id, ''
                if len(contig.alignments) > 0:
                    for i, alignment in enumerate(contig.alignments):
                        accession = alignment.accession

                        # Retrieve taxid identifier from local Blast database
                        command = 'blastdbcmd -entry {} -db {}  -outfmt "%T"'.format(accession, params.blast_db)
                        output = subprocess.check_output(command.split()).strip().decode('utf-8')
                        taxid = int(output.split('\n')[0].replace('"', ''))
                        if taxid not in taxes:
                            continue
                        align_taxonomy = ';'.join(taxes[taxid])
                        if is_first:
                            taxonomy = align_taxonomy
                            is_first = False
                        is_classified = not ('uncultured' in align_taxonomy.lower() or 'environmental' in align_taxonomy.lower())
                        if is_classified:
                            taxonomy = align_taxonomy
                            break

                outtax.write('{seq_id}\t{taxonomy}\n'.format(seq_id=seq_id, taxonomy=taxonomy))