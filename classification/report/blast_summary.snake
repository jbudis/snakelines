include: config['snake_dir'] + '/classification/classifiers/blast.snake'

import subprocess
from Bio.Blast import NCBIXML
from Bio import SeqIO
import pysam
import os
import numpy as np
import pickle
import pandas as pd
pd.set_option('display.max_colwidth', 1000000)
pd.options.display.float_format = '{:.3f}'.format

DEFAULT_MIN_QUERY_COVERAGE = 0.5
DEFAULT_MAX_SUBJECT_SEQS = 10

n_contigs = int(config['blast']['report']['max_query_seqs'])
min_similarity = float(config['blast']['report']['min_query_coverage']) if 'min_query_coverage' in config['blast']['report'] else DEFAULT_MIN_QUERY_COVERAGE
max_subject_seqs = float(config['blast']['report']['max_subject_seqs']) if 'max_subject_seqs' in config['blast']['report'] else DEFAULT_MAX_SUBJECT_SEQS

def create_summary(input, output, params):

        # Split contigs.fasta file into separate fasta files, one for each contig - downloadable from HTML page
        if not os.path.exists(params.contig_dir):
            os.mkdir(params.contig_dir)

        contig_lens = {}
        for i, contig in enumerate(SeqIO.parse(input.contigs, 'fasta')):
            with open('%s/%s.fa' % (params.contig_dir, contig.id), 'w') as out:
                SeqIO.write(contig, out, 'fasta')
                contig_lens[contig.id] = len(contig.seq)
            if i >= n_contigs:
                break

        # Collect Blast results
        contig_items = []

        def iter_contigs(blast):
            last_qname, alignments = None, []

            with open(blast) as blast_results:
                for i, line in enumerate(blast_results):
                    items = line.strip().split("\t")
                    if i == 0:
                        tax_col = items.index('taxonomy')
                        gbid_col = items.index('sacc')
                        sname_col = items.index('stitle')
                        qname_col = items.index('qseqid')
                        qstart_col = items.index('qstart')
                        qend_col = items.index('qend')
                        continue

                    qname = items[qname_col]
                    if qname not in contig_lens:
                        continue

                    if last_qname and qname != last_qname:
                        yield (last_qname, contig_lens[last_qname]), alignments
                        last_qname, alignments = qname, []

                    last_qname = qname

                    qcov = (int(items[qend_col]) - int(items[qstart_col])) / contig_lens[qname]
                    if qcov < min_similarity:
                        continue

                    if len(alignments) >= max_subject_seqs:
                        continue

                    alignments.append((items[gbid_col], items[tax_col], items[sname_col], qcov))

            yield (last_qname, contig_lens[last_qname]), alignments

        with open(input.blast) as blast_results:
            for (contig_name, contig_len), alignments in iter_contigs(input.blast):

                links = []
                contig_link = '<a href="individual/%s.fa">%s</a>' % (contig_name, contig_name)
                for gbid, tax, sname, qcov in alignments:
                    links.append('%3.2f%% - <a href="https://www.ncbi.nlm.nih.gov/nuccore/%s", title="%s">%s</a>' % \
                                  (qcov * 100, gbid, tax, sname))

                contig_info = {'Contig': contig_link,
                               'Homologues': '<br/>'.join(links),
                               'Length': contig_len,
                               'Contig Name': contig_name}

                contig_items.append(contig_info)

        # Generate report
        contigs = pd.DataFrame(contig_items)
        columns = ['Contig', 'Length', 'Homologues']

        if hasattr(input, 'remap'):
            bam = pysam.AlignmentFile(input.remap, 'rb')
            columns.insert(2, 'Mapped reads')
            contigs['Mapped reads'] = contigs['Contig Name'].apply(lambda contig_name: bam.count(contig=contig_name))
            bam.close()

        if hasattr(input, 'pvalues'):
            pvalues = pd.read_csv(input.pvalues, sep='\t', index_col=0)
            columns.insert(2, 'Viral pvalue')
            contigs['Viral pvalue'] = contigs['Contig Name'].apply(lambda cid: pvalues.get_value(index=cid, col='pvalue'))

        contigs = contigs[columns]
        contigs.index = np.arange(1, len(contigs) + 1)
        contigs_table = contigs.to_html(escape=False).replace('<table ', '<table id="data" ')

        TEMPLATE = open(input.template).read()
        with open(output.report, 'w') as out:
            out.write(TEMPLATE.format(contigs_table=contigs_table, contigs_fa=params.contig_fa))

        os.system('cp %s %s' % (input.contigs, output.contigs))

rule blast_summary:
    input:
        contigs = '{indir}/{name}.fa',
        blast = '{indir}/blast/{name}.blast.tax.tsv',
        remap = '{indir}/mapping/{name}.bam',
        remap_idx = '{indir}/mapping/{name}.bam.bai',
        template = srcdir('templates/abundances.html')
    output:
        report = '{indir}/blast/{name}.html',
        contigs = '{indir}/blast/individual/{name}.fa'
    params:
        contig_dir = '{indir}/blast/individual',
        contig_fa = 'individual/{name}.fa'
    run:
        create_summary(input, output, params)

rule blast_summary_without_coverage:
    input:
        contigs = '{indir}/{name}.fa',
        blast = '{indir}/blast/{name}.blast.tax.tsv',
        template = srcdir('templates/abundances.html')
    output:
        report = '{indir}/blast-nocoverage/{name}.html',
        contigs = '{indir}/blast-nocoverage/individual/{name}.fa'
    params:
        contig_dir = '{indir}/blast-nocoverage/individual',
        contig_fa = 'individual/{name}.fa'
    run:
        create_summary(input, output, params)

rule blast_summary_viral:
    input:
        contigs = '{indir}/{name}.fa',
        blast = '{indir}/blast/{name}.blast.tax.tsv',
        template = srcdir('templates/abundances.html'),
        pvalues = '{indir}/virfinder/{name}.pvalues.tsv'
    output:
        report = '{indir}/blast-viral/{name}.html',
        contigs = '{indir}/blast-viral/individual/{name}.fa'
    params:
        contig_dir = '{indir}/blast-viral/individual',
        contig_fa = 'individual/{name}.fa'
    run:
        create_summary(input, output, params)


rule blast_assign_taxonomy:
    input:
        fa = '{indir}/{name}.fa',
        blast = '{indir}/blast/{name}.blast',
        tax_db = '/data/genome/metagenome/blast/nt/17-01-17/taxonomy/code_taxonomy.pickle'
    output:
        fa = '{indir}/{name}.tax'
    params:
        blast_db = '/data/genome/metagenome/blast/nt/17-01-17/nt'
    run:
        taxes = pickle.load(open(input.tax_db, 'rb'))
        with open(input.blast) as blast, \
             open(input.fa) as infa, \
             open(output.fa, 'w') as outtax:
            is_first = True
            for i, (contig, sequence) in enumerate(zip(NCBIXML.parse(blast), SeqIO.parse(infa, 'fasta'))):
                assert contig.query.split()[0] == sequence.id, contig.query + ' - ' + sequence.id

                seq_id, taxonomy = sequence.id, ''
                if len(contig.alignments) > 0:
                    for i, alignment in enumerate(contig.alignments):
                        accession = alignment.accession

                        # Retrieve taxid identifier from local Blast database
                        command = 'blastdbcmd -entry {} -db {}  -outfmt "%T"'.format(accession, params.blast_db)
                        output = subprocess.check_output(command.split()).strip().decode('utf-8')
                        taxid = int(output.split('\n')[0].replace('"', ''))
                        if taxid not in taxes:
                            continue
                        align_taxonomy = ';'.join(taxes[taxid])
                        if is_first:
                            taxonomy = align_taxonomy
                            is_first = False
                        is_classified = not ('uncultured' in align_taxonomy.lower() or 'environmental' in align_taxonomy.lower())
                        if is_classified:
                            taxonomy = align_taxonomy
                            break

                outtax.write('{seq_id}\t{taxonomy}\n'.format(seq_id=seq_id, taxonomy=taxonomy))