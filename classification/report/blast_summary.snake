include: config['snake_dir'] + '/classification/classifiers/blast.snake'

import subprocess
from Bio.Blast import NCBIXML
from Bio import SeqIO
import os
import numpy as np
import pickle
import pandas as pd
pd.set_option('display.max_colwidth', 1000000)
pd.options.display.float_format = '{:.1f}'.format

MISSING_COVERAGE = ''

n_contigs = int(config['blast']['report']['max_query_seqs'])

# Calculation of query/target overlap
def deduplicate(intervals):
    dedup = []
    for start, end in sorted(intervals):
        if dedup and dedup[-1][1] >= start - 1:
            dedup[-1][1] = max(dedup[-1][1], end)
        else:
            dedup.append([start, end])

    return dedup

def calc_align_len(alignment):
    intervals = deduplicate([(hsp.query_start, hsp.query_end) for hsp in alignment.hsps])
    return sum(end - start + 1 for start, end in intervals)

rule blast_summary:
    input:
        contigs = '{indir}/{name}.fa',
        blast = '{indir}/blast/{name}.blast',
        template = srcdir('templates/abundances.html')
    output:
        report = '{indir}/blast/{name}.html',
        contigs = '{indir}/blast/individual/{name}.fa'
    params:
        contig_dir = '{indir}/blast/individual',
        contig_fa = 'individual/{name}.fa'
    run:

        # Split contigs.fasta file into separate fasta files, one for each contig - downloadable from HTML page
        if not os.path.exists(params.contig_dir):
            os.mkdir(params.contig_dir)

        for i, contig in enumerate(SeqIO.parse(input.contigs, 'fasta')):
            with open('%s/%s.fa' % (params.contig_dir, contig.id), 'w') as out:
                SeqIO.write(contig, out, 'fasta')
            if i >= n_contigs:
                break



        # Collect Blast results
        contig_items = []
        with open(input.blast) as blast_results:
            for i, contig in enumerate(NCBIXML.parse(blast_results)):

                contig_link = '<a href="individual/%s.fa">%s</a>' % (contig.query, contig.query)

                contig_len = contig.query_length
                aligns = contig.alignments
                align_lens = map(calc_align_len, aligns)
                titles = [align.title for align in aligns]
                gbids = [title.split('|')[3] for title in titles]
                names = [title.split('|')[-1] for title in titles]
                links = []
                for gbid, name, align_len in zip(gbids, names, align_lens):
                    coverage = align_len / contig_len
                    links.append('%3.2f%% - <a href="https://www.ncbi.nlm.nih.gov/nuccore/%s">%s</a>' % \
                                  (coverage * 100, gbid, name))

                # Unicycler
                if 'depth=' in contig.query:
                    coverage = float(contig.query.split('depth=')[1].split('x')[0])
                # Spades
                elif '_cov_' in contig.query:
                    coverage = float(contig.query.split('_')[5])
                else:
                    coverage = MISSING_COVERAGE


                contig_info = {'Contig': contig_link,
                               'Homologues': '<br/>'.join(links),
                               'Length': contig_len,
                               'Coverage': coverage,
                               'Genbank': gbids[0] if len(gbids) > 0 else ''}

                contig_items.append(contig_info)

                if i >= n_contigs:
                    break

        # Generate report
        columns = ['Contig', 'Genbank', 'Length', 'Coverage', 'Homologues']
        contigs = pd.DataFrame(contig_items)[columns]
        if list(contigs['Coverage'].unique()) == [MISSING_COVERAGE]:
            contigs = contigs[[col for col in contigs.columns if col != 'Coverage']]

        contigs.index = np.arange(1, len(contigs) + 1)
        contigs_table = contigs.to_html(escape=False).replace('<table ', '<table id="data" ')


        TEMPLATE = open(input.template).read()
        with open(output.report, 'w') as out:
            out.write(TEMPLATE.format(contigs_table=contigs_table, contigs_fa=params.contig_fa))

        os.system('cp %s %s' % (input.contigs, output.contigs))


rule blast_assign_taxonomy:
    input:
        fa = '{indir}/{name}.fa',
        blast = '{indir}/blast/{name}.blast',
        tax_db = '/data/genome/metagenome/blast/nt/17-01-17/taxonomy/code_taxonomy.pickle'
    output:
        fa = '{indir}/{name}.tax'
    params:
        blast_db = '/data/genome/metagenome/blast/nt/17-01-17/nt'
    run:
        taxes = pickle.load(open(input.tax_db, 'rb'))
        with open(input.blast) as blast, \
             open(input.fa) as infa, \
             open(output.fa, 'w') as outtax:
            for i, (contig, sequence) in enumerate(zip(NCBIXML.parse(blast), SeqIO.parse(infa, 'fasta'))):
                assert contig.query.split()[0] == sequence.id, contig.query + ' - ' + sequence.id

                seq_id, taxonomy = sequence.id, ''
                if len(contig.alignments) > 0:
                    accession = contig.alignments[0].accession

                    # Retrieve taxid identifier from local Blast database
                    command = 'blastdbcmd -entry {} -db {}  -outfmt "%T"'.format(accession, params.blast_db)
                    taxid = int(subprocess.check_output(command.split()).strip()[1:-1].decode('utf-8'))
                    taxonomy = ';'.join(taxes[taxid])

                outtax.write('{seq_id}\t{taxonomy}\n'.format(seq_id=seq_id, taxonomy=taxonomy))