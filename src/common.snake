import sys
import tempfile
from pipeline.samples import Pipeline
from config.impute_mapping_types import impute_mapping_types, impute_read_types
from config.load_imports import import_files_in_config
from config.values_retrieval import extract_from_config

def configured_temp(output_file):
    """
    Choose if Fastq file should be marker by snakemake temporary flag temp() based on configuration.
    This function cannot(??) be made into pure python.

    :param output_file: output read file of a rule
    :return: temporary output or normal output
    """
    return temp(output_file) if method_config.get('temporary', False) else output_file

# create pipeline
pipeline = Pipeline()

DEFAULT_SAMPLE_CONFIGURATION = [{'name': '.*'}]
DEFAULT_SAMPLE_LOCATION = 'reads/original/{}_R1.fastq.gz'
sample_configurations = config.get('samples', DEFAULT_SAMPLE_CONFIGURATION)
for sample_configuration in sample_configurations:

    # Filter samples according to the user configuration
    samples = []
    if 'name' in sample_configuration:
        wildcards = glob_wildcards('reads/original/{sample, %s}_R1.fastq.gz' % sample_configuration['name'])
        assert len(wildcards.sample) > 0, 'No samples in reads/original/ match configured pattern {}'.format(sample_configuration['name'])
        samples = wildcards.sample

    reference = sample_configuration.get('reference', None)
    pipeline.add(samples, reference,
                 sample_configuration.get('panel', None),
                 sample_configuration.get('prebuilt', False),
                 bool(extract_from_config(config, ['reference', 'download', reference], [])))

if pipeline.is_empty():
    print('No samples to analyse', file=sys.stderr)

# impute types:
impute_mapping_types(pipeline, config)
impute_read_types(pipeline, config)

# add tmp dir:
if not 'tmp_dir' in config:
    config['tmp_dir'] = tempfile.gettempdir()

# load imports
rules_dir = '{}/rules'.format(config['snakelines_dir'])
for import_file, method_config in import_files_in_config(config, rules_dir):
    assert os.path.isfile(import_file), 'Configured snake file {} does not exist'.format(import_file)
    print('Importing snakefile:', import_file, file=sys.stderr)
    include: import_file

# Find in configuration map post-process types for which should be report generated
map_types_with_quality_report = extract_from_config(config, ['mapping', 'report', 'quality_report', 'map_types'], [])

# References with at least 2 samples
multisample_references = [reference for reference in pipeline.references if len(pipeline.samples_for(reference)) > 1]

# Find in configuration read preprocess types that should be generated
preprocess = extract_from_config(config, ['reads', 'preprocess'], {})
read_types_to_keep = [read_type for read_type in preprocess.keys() if not preprocess[read_type].get('temporary', True)]

# Find in configuration read preprocess types for which should be report generated
read_types_with_quality_report = extract_from_config(config, ['reads', 'report', 'quality_report', 'read_types'], [])
map_types_with_quality_report = extract_from_config(config, ['mapping', 'report', 'quality_report', 'map_types'], [])

# Which taxonomic levels should be extracted in separate tables
tax_levels_count_table = extract_from_config(config, ['classification', 'report', 'taxonomic_counts', 'count_table', 'tax_levels'], [])
tax_levels_count_table.append('alltaxes')

# Which taxonomic levels should be visualised in barplots, and in which image formats
tax_levels_barplot = extract_from_config(config, ['classification', 'report', 'taxonomic_counts', 'barplot', 'tax_levels'], [])
barplot_formats = extract_from_config(config, ['classification', 'report', 'taxonomic_counts', 'barplot', 'formats'], ['png'])

# Which taxonomic levels should be computed as alpha_diversity
tax_levels_alphas = extract_from_config(config, ['classification', 'report', 'taxonomic_counts', 'alpha_diversity', 'tax_levels'], [])

pca_formats = extract_from_config(config, ['classification', 'report', 'transcripts', 'pca', 'formats'], ['svg', 'png'])
